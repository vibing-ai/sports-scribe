"""
Logging configuration for SportsScribe pipeline.

This module provides centralized logging configuration for all pipeline components.
"""

import logging
import sys
from typing import Optional
from pathlib import Path


def setup_logging(
    level: str = "INFO",
    log_file: Optional[str] = None,
    include_debug: bool = False
) -> None:
    """
    Setup logging configuration for the SportsScribe pipeline.
    
    Args:
        level: Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
        log_file: Optional file path to write logs to
        include_debug: Whether to include debug logs in file output
    """
    # Convert string level to logging constant
    numeric_level = getattr(logging, level.upper(), logging.INFO)
    
    # Create formatter
    console_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    file_formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Setup root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG if include_debug else numeric_level)
    
    # Clear existing handlers
    root_logger.handlers.clear()
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(numeric_level)
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # File handler (if specified)
    if log_file:
        # Ensure log directory exists
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG if include_debug else numeric_level)
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    
    # Set specific logger levels
    loggers_to_configure = [
        'scriber_agents.pipeline',
        'scriber_agents.data_collector',
        'scriber_agents.researcher',
        'scriber_agents.writer',
        'openai',
        'aiohttp',
        'urllib3'
    ]
    
    for logger_name in loggers_to_configure:
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.DEBUG if include_debug else numeric_level)
        logger.propagate = True
    
    # Reduce noise from external libraries
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('aiohttp').setLevel(logging.WARNING)
    
    logging.info(f"ðŸ”§ Logging configured - Level: {level}, File: {log_file or 'None'}, Debug: {include_debug}")


def get_logger(name: str) -> logging.Logger:
    """
    Get a logger instance with the specified name.
    
    Args:
        name: Logger name (usually __name__)
        
    Returns:
        Configured logger instance
    """
    return logging.getLogger(name)


def log_pipeline_start(operation: str, **kwargs) -> None:
    """
    Log the start of a pipeline operation.
    
    Args:
        operation: Name of the operation
        **kwargs: Additional context information
    """
    logger = logging.getLogger('scriber_agents.pipeline')
    context = ", ".join([f"{k}={v}" for k, v in kwargs.items()])
    logger.info(f"[PIPELINE] Starting {operation} - {context}")


def log_pipeline_step(step: str, **kwargs) -> None:
    """
    Log a pipeline step.
    
    Args:
        step: Name of the step
        **kwargs: Additional context information
    """
    logger = logging.getLogger('scriber_agents.pipeline')
    context = ", ".join([f"{k}={v}" for k, v in kwargs.items()])
    logger.info(f"[PIPELINE] Step: {step} - {context}")


def log_pipeline_success(operation: str, duration: float, **kwargs) -> None:
    """
    Log successful completion of a pipeline operation.
    
    Args:
        operation: Name of the operation
        duration: Duration in seconds
        **kwargs: Additional context information
    """
    logger = logging.getLogger('scriber_agents.pipeline')
    context = ", ".join([f"{k}={v}" for k, v in kwargs.items()])
    logger.info(f"[PIPELINE] {operation} completed successfully in {duration:.2f}s - {context}")


def log_pipeline_error(operation: str, error: Exception, duration: float, **kwargs) -> None:
    """
    Log an error in a pipeline operation.
    
    Args:
        operation: Name of the operation
        error: The exception that occurred
        duration: Duration in seconds
        **kwargs: Additional context information
    """
    logger = logging.getLogger('scriber_agents.pipeline')
    context = ", ".join([f"{k}={v}" for k, v in kwargs.items()])
    logger.error(f"[PIPELINE] {operation} failed after {duration:.2f}s - {error} - {context}")


def log_data_collection(source: str, **kwargs) -> None:
    """
    Log data collection operations.
    
    Args:
        source: Data source name
        **kwargs: Additional context information
    """
    logger = logging.getLogger('scriber_agents.data_collector')
    context = ", ".join([f"{k}={v}" for k, v in kwargs.items()])
    logger.info(f"[COLLECTOR] Collecting from {source} - {context}")


def log_research_operation(operation: str, **kwargs) -> None:
    """
    Log research operations.
    
    Args:
        operation: Research operation name
        **kwargs: Additional context information
    """
    logger = logging.getLogger('scriber_agents.researcher')
    context = ", ".join([f"{k}={v}" for k, v in kwargs.items()])
    logger.info(f"[RESEARCHER] {operation} - {context}")


def log_writing_operation(article_type: str, **kwargs) -> None:
    """
    Log writing operations.
    
    Args:
        article_type: Type of article being written
        **kwargs: Additional context information
    """
    logger = logging.getLogger('scriber_agents.writer')
    context = ", ".join([f"{k}={v}" for k, v in kwargs.items()])
    logger.info(f"[WRITER] Generating {article_type} - {context}")


# Default configuration
if __name__ == "__main__":
    setup_logging(level="INFO", include_debug=False) 